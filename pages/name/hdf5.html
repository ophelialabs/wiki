    <div class="content-section">
        <header>
            <h1>HDF5 & HDF-EOS Data Implementation Guide</h1>
            <p class="lead">Enterprise-Grade Data Processing for Quantum IoT with Python & Pandas</p>
            <a href="https://space.skyrocket.de/doc_sdat/lacunasat-4.htm" target="_blank" rel="noopenner noreferrer" class="link-card">Gunter's Space page</a>
        </header>

        <nav>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#hdf5-basics">HDF5 Basics</a></li>
                <li><a href="#data-formats">Data Formats</a></li>
                <li><a href="#pandas-integration">Pandas Integration</a></li>
                <li><a href="#python-implementation">Python Implementation</a></li>
                <li><a href="#best-practices">Best Practices</a></li>
                <li><a href="#references">References</a></li>
            </ul>
        </nav>

            <section id="overview">
                <h2>Overview</h2>
                <p>
                    The Quantum IoT system uses <strong>HDF5</strong> (Hierarchical Data Format 5) and <strong>HDF-EOS</strong> 
                    (HDF-EOS for Earth Observing System) as primary data storage formats. These standards are used by major 
                    space missions including:
                </p>
                <ul>
                    <li><strong>Orbiting Carbon Observatory 2 (OCO-2)</strong> - Atmospheric CO2 measurements</li>
                    <li><strong>Joint Polar Satellite System (JPSS)</strong> - Weather and climate data</li>
                    <li><strong>Landsat 8/9</strong> - Land surface imaging</li>
                    <li><strong>MODIS</strong> - Moderate Resolution Imaging Spectroradiometer data</li>
                </ul>
                <div class="info-box">
                    <strong>Learn More:</strong> <a href="https://www.earthdata.nasa.gov/about/esdis/esco/standards-practices/hdf5" target="_blank">NASA Earthdata HDF5 Standards</a>
                </div>
            </section>

            <section id="hdf5-basics">
                <h2>HDF5 Format Fundamentals</h2>
                
                <h3>What is HDF5?</h3>
                <p>
                    HDF5 is a flexible data format designed to store and organize large amounts of complex data. 
                    It supports a variety of data types and provides hierarchical organization similar to file systems.
                </p>

                <h3>Key Characteristics</h3>
                <table>
                    <tr>
                        <th>Feature</th>
                        <th>Description</th>
                        <th>Benefit</th>
                    </tr>
                    <tr>
                        <td>Hierarchical Structure</td>
                        <td>Groups and datasets organized in tree-like format</td>
                        <td>Intuitive data organization</td>
                    </tr>
                    <tr>
                        <td>Compression</td>
                        <td>Multiple compression algorithms supported</td>
                        <td>Reduces file size by 10-90%</td>
                    </tr>
                    <tr>
                        <td>Metadata</td>
                        <td>Extensive attribute support</td>
                        <td>Data documentation built-in</td>
                    </tr>
                    <tr>
                        <td>Scalability</td>
                        <td>Supports multi-petabyte files</td>
                        <td>Future-proof for large datasets</td>
                    </tr>
                    <tr>
                        <td>Performance</td>
                        <td>Chunked storage and partial I/O</td>
                        <td>Efficient data access patterns</td>
                    </tr>
                </table>

                <h3>HDF5 Structure Example</h3>
                <pre><code>/ (root)
├── /mission_data (Group)
│   ├── /mission_data/telemetry (Dataset)
│   ├── /mission_data/quantum_states (Dataset)
│   └── @mission_id="OCO-2" (Attribute)
├── /metadata (Group)
│   ├── /metadata/calibration (Dataset)
│   └── /metadata/validation (Dataset)
└── /timestamps (Group)
    ├── /timestamps/utc (Dataset)
    └── /timestamps/tai (Dataset)</code></pre>
            </section>

            <section id="data-formats">
                <h2>HDF-EOS Data Formats</h2>
                
                <h3>Grid Format</h3>
                <div class="subsection">
                    <h4>Purpose</h4>
                    <p>Stores data on regular or irregular grids with coordinate information.</p>
                    <h4>Use Cases</h4>
                    <ul>
                        <li>Global climate models</li>
                        <li>Satellite imagery (geographically gridded)</li>
                        <li>Quantum state lattices</li>
                    </ul>
                    <h4>Structure</h4>
                    <pre><code>Grid Structure:
  - Origin point (upper left)
  - Grid dimensions (rows × columns)
  - Pixel size (degrees or meters)
  - Coordinate system (e.g., WGS84)
  - Data fields (temperature, pressure, quantum amplitude)</code></pre>
                </div>

                <h3>Swath Format</h3>
                <div class="subsection">
                    <h4>Purpose</h4>
                    <p>Represents data collected along satellite orbital tracks with irregular spatial coverage.</p>
                    <h4>Use Cases</h4>
                    <ul>
                        <li>Satellite sensor measurements</li>
                        <li>Instrument scans</li>
                        <li>Time-series quantum measurements</li>
                    </ul>
                    <h4>Structure</h4>
                    <pre><code>Swath Structure:
  - Geolocation fields (latitude, longitude, altitude)
  - Data fields (measurements)
  - Time information
  - Dimension maps (relating variables)</code></pre>
                </div>

                <h3>Point Format</h3>
                <div class="subsection">
                    <h4>Purpose</h4>
                    <p>Stores data at discrete point locations with varying attributes.</p>
                    <h4>Use Cases</h4>
                    <ul>
                        <li>Weather station data</li>
                        <li>Ground-based measurements</li>
                        <li>Sparse quantum state samples</li>
                    </ul>
                </div>
            </section>

            <section id="pandas-integration">
                <h2>Python Pandas Integration for Data Transformation</h2>

                <h3>Why Pandas for HDF5?</h3>
                <div class="highlight">
                    <strong>Pandas Benefits:</strong>
                    <ul>
                        <li>Intuitive DataFrame operations</li>
                        <li>Built-in HDF5 support via <code>HDFStore</code></li>
                        <li>Efficient data filtering and aggregation</li>
                        <li>Easy integration with machine learning libraries</li>
                        <li>Time-series capabilities for temporal data</li>
                    </ul>
                </div>

                <h3>Installation & Setup</h3>
                <pre><code># Install required packages
pip install pandas tables h5py numpy scipy

# Import libraries
import pandas as pd
import h5py
import numpy as np
from pathlib import Path</code></pre>

                <h3>Reading HDF5 Files with Pandas</h3>
                <h4>Method 1: Using HDFStore</h4>
                <pre><code># Open HDF5 file as HDFStore
store = pd.HDFStore('satellite_data.h5', mode='r')

# List all keys in the store
print(store.keys())

# Read specific dataset
df = store['/mission_data/telemetry']

# Display basic information
print(df.info())
print(df.head())
print(df.describe())

# Close the store
store.close()</code></pre>

                <h4>Method 2: Using h5py for Lower-Level Access</h4>
                <pre><code>import h5py

# Open HDF5 file
with h5py.File('satellite_data.h5', 'r') as hdf_file:
    # Navigate through groups
    mission_group = hdf_file['/mission_data']
    
    # Access dataset
    telemetry_data = mission_group['telemetry'][:]
    
    # Read attributes
    mission_id = mission_group.attrs['mission_id']
    
    # Convert to DataFrame
    df = pd.DataFrame(telemetry_data)
    
    # Add metadata as columns
    df['mission_id'] = mission_id
    df['source'] = 'OCO-2'</code></pre>

                <h3>Data Transformation Operations</h3>
                
                <h4>1. Filtering and Selection</h4>
                <pre><code># Filter by quality flags
df_good_quality = df[df['quality_flag'] == 0]

# Select time range
df_2024 = df[(df['timestamp'] >= '2024-01-01') & 
             (df['timestamp'] < '2024-12-31')]

# Subset columns
essential_cols = df[['latitude', 'longitude', 'measurement', 'uncertainty']]

# Boolean indexing
high_values = df[df['measurement'] > df['measurement'].quantile(0.75)]</code></pre>

                <h4>2. Data Aggregation and Grouping</h4>
                <pre><code># Group by geographic region and compute statistics
regional_stats = df.groupby('region')['measurement'].agg([
    'mean',
    'std',
    'min',
    'max',
    'count'
])

# Time-based aggregation (resample)
daily_average = df.set_index('timestamp').resample('D')['measurement'].mean()

# Multi-level grouping
seasonal_regional = df.groupby(['season', 'region'])['measurement'].apply([
    lambda x: x.mean(),
    lambda x: x.std(),
    lambda x: x.quantile(0.5)
])</code></pre>

                <h4>3. Data Cleaning and Normalization</h4>
                <pre><code># Handle missing values
df_clean = df.dropna(subset=['measurement', 'quality_flag'])

# Interpolate missing values
df['measurement'] = df['measurement'].interpolate(method='linear')

# Normalize to [0, 1]
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df['measurement_normalized'] = scaler.fit_transform(df[['measurement']])

# Standardize (z-score)
df['measurement_std'] = (df['measurement'] - df['measurement'].mean()) / df['measurement'].std()

# Detect and handle outliers
Q1 = df['measurement'].quantile(0.25)
Q3 = df['measurement'].quantile(0.75)
IQR = Q3 - Q1
df_clean = df[(df['measurement'] >= Q1 - 1.5*IQR) & 
              (df['measurement'] <= Q3 + 1.5*IQR)]</code></pre>

                <h4>4. Merging and Joining Datasets</h4>
                <pre><code># Merge datasets from multiple sources
merged_df = pd.merge(
    classical_data,
    quantum_data,
    on=['timestamp', 'location_id'],
    how='inner',
    suffixes=('_classical', '_quantum')
)

# Concatenate multiple HDF5 files
dfs = []
for file_path in Path('.').glob('satellite_*.h5'):
    with pd.HDFStore(file_path, mode='r') as store:
        dfs.append(store['/mission_data/telemetry'])

combined_df = pd.concat(dfs, ignore_index=True)</code></pre>

                <h4>5. Feature Engineering for ML</h4>
                <pre><code># Extract time features
df['hour'] = pd.to_datetime(df['timestamp']).dt.hour
df['day_of_week'] = pd.to_datetime(df['timestamp']).dt.dayofweek
df['month'] = pd.to_datetime(df['timestamp']).dt.month

# Create rolling statistics
df['measurement_ma7'] = df['measurement'].rolling(window=7).mean()
df['measurement_std7'] = df['measurement'].rolling(window=7).std()

# Lag features
df['measurement_lag1'] = df['measurement'].shift(1)
df['measurement_lag7'] = df['measurement'].shift(7)

# Difference features
df['measurement_diff'] = df['measurement'].diff()
df['measurement_pct_change'] = df['measurement'].pct_change()

# Categorical encoding
df['region_encoded'] = pd.Categorical(df['region']).codes</code></pre>

                <h4>6. Writing Transformed Data to HDF5</h4>
                <pre><code># Write DataFrame to HDF5
with pd.HDFStore('processed_data.h5', mode='w') as store:
    # Store with compression
    store.put('telemetry_processed', df, format='table', complevel=9, complib='blosc')
    
    # Store with metadata
    store.get_storer('telemetry_processed').attrs.metadata = {
        'processing_date': pd.Timestamp.now(),
        'processing_version': '1.0',
        'transformations_applied': ['normalization', 'outlier_removal']
    }

# Or use h5py for custom hierarchical structure
import h5py

with h5py.File('processed_data.h5', 'w') as hdf_file:
    group = hdf_file.create_group('processed/mission_data')
    
    # Store data with compression
    dset = group.create_dataset(
        'telemetry_processed',
        data=df.values,
        compression='gzip',
        compression_opts=9,
        chunks=(1000, df.shape[1])
    )
    
    # Add metadata
    dset.attrs['columns'] = list(df.columns)
    dset.attrs['processing_date'] = str(pd.Timestamp.now())
    dset.attrs['records'] = len(df)</code></pre>
            </section>

            <section id="python-implementation">
                <h2>Complete Python Implementation Example</h2>

                <h3>Workflow: Reading, Processing, and Storing Satellite Data</h3>
                <pre><code>import pandas as pd
import h5py
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path

class HDF5DataProcessor:
    """Process satellite HDF5 data with pandas"""
    
    def __init__(self, input_file, output_file):
        self.input_file = input_file
        self.output_file = output_file
        
    def read_satellite_data(self):
        """Read data from HDF5 file"""
        with h5py.File(self.input_file, 'r') as hdf_file:
            # Navigate groups
            mission_group = hdf_file['/mission_data']
            
            # Extract datasets
            telemetry = mission_group['telemetry'][:]
            timestamps = mission_group['timestamps'][:]
            quality_flags = mission_group['quality_flags'][:]
            
            # Create DataFrame
            df = pd.DataFrame({
                'timestamp': pd.to_datetime(timestamps, unit='s'),
                'latitude': telemetry[:, 0],
                'longitude': telemetry[:, 1],
                'measurement': telemetry[:, 2],
                'uncertainty': telemetry[:, 3],
                'quality_flag': quality_flags
            })
            
            # Store attributes
            self.metadata = {
                'mission': mission_group.attrs.get('mission_id', 'Unknown'),
                'version': mission_group.attrs.get('version', '1.0')
            }
            
            return df
    
    def clean_and_transform(self, df):
        """Apply data cleaning and transformation"""
        # Remove low quality data
        df_clean = df[df['quality_flag'] == 0].copy()
        
        # Remove duplicates
        df_clean = df_clean.drop_duplicates(subset=['timestamp', 'latitude', 'longitude'])
        
        # Interpolate missing values
        df_clean['measurement'] = df_clean['measurement'].interpolate(method='linear')
        
        # Detect outliers using IQR
        Q1 = df_clean['measurement'].quantile(0.25)
        Q3 = df_clean['measurement'].quantile(0.75)
        IQR = Q3 - Q1
        df_clean = df_clean[
            (df_clean['measurement'] >= Q1 - 1.5*IQR) & 
            (df_clean['measurement'] <= Q3 + 1.5*IQR)
        ]
        
        # Normalize measurements
        df_clean['measurement_normalized'] = (
            (df_clean['measurement'] - df_clean['measurement'].mean()) / 
            df_clean['measurement'].std()
        )
        
        # Add temporal features
        df_clean['hour'] = df_clean['timestamp'].dt.hour
        df_clean['day_of_week'] = df_clean['timestamp'].dt.dayofweek
        df_clean['month'] = df_clean['timestamp'].dt.month
        
        # Add spatial aggregates
        df_clean['lat_bin'] = pd.cut(df_clean['latitude'], bins=10)
        df_clean['lon_bin'] = pd.cut(df_clean['longitude'], bins=10)
        
        return df_clean
    
    def aggregate_statistics(self, df):
        """Generate aggregated statistics"""
        # Spatial statistics
        spatial_stats = df.groupby(['lat_bin', 'lon_bin']).agg({
            'measurement': ['mean', 'std', 'count'],
            'uncertainty': 'mean'
        })
        
        # Temporal statistics
        temporal_stats = df.groupby('hour').agg({
            'measurement': ['mean', 'std'],
            'uncertainty': 'mean'
        })
        
        return spatial_stats, temporal_stats
    
    def save_processed_data(self, df, spatial_stats, temporal_stats):
        """Save processed data to HDF5"""
        with pd.HDFStore(self.output_file, mode='w') as store:
            # Store main data
            store.put('data/telemetry', df, format='table', complevel=9)
            
            # Store statistics
            store.put('statistics/spatial', spatial_stats, format='table')
            store.put('statistics/temporal', temporal_stats, format='table')
            
            # Store metadata
            store.get_storer('data/telemetry').attrs.metadata = self.metadata
        
        print(f"Data saved to {self.output_file}")
    
    def process(self):
        """Execute full processing pipeline"""
        print("Reading satellite data...")
        df = self.read_satellite_data()
        print(f"  Loaded {len(df)} records")
        
        print("Cleaning and transforming data...")
        df_clean = self.clean_and_transform(df)
        print(f"  Cleaned data: {len(df_clean)} records remaining")
        
        print("Computing statistics...")
        spatial_stats, temporal_stats = self.aggregate_statistics(df_clean)
        
        print("Saving processed data...")
        self.save_processed_data(df_clean, spatial_stats, temporal_stats)
        
        return df_clean, spatial_stats, temporal_stats

# Usage
if __name__ == "__main__":
    processor = HDF5DataProcessor(
        input_file='raw_satellite_data.h5',
        output_file='processed_satellite_data.h5'
    )
    
    df_processed, spatial_stats, temporal_stats = processor.process()
    
<p><strong>Last Updated:</strong> December 21, 2025</p>
    </div>

                <h3>Advanced: Integration with Quantum Data</h3>
                <pre><code># Integrate classical and quantum data
def merge_quantum_classical_data(classical_df, quantum_state_file):
    """Merge classical satellite data with quantum states"""
    
    # Read quantum states from HDF5
    with h5py.File(quantum_state_file, 'r') as qf:
        quantum_states = qf['/quantum/states'][:]
        quantum_times = qf['/quantum/timestamps'][:]
    
    # Create quantum DataFrame
    quantum_df = pd.DataFrame({
        'timestamp': pd.to_datetime(quantum_times, unit='s'),
        'quantum_amplitude': quantum_states[:, 0],
        'quantum_phase': quantum_states[:, 1],
        'fidelity': quantum_states[:, 2]
    })
    
    # Merge on timestamp (nearest match within tolerance)
    merged = pd.merge_asof(
        classical_df.sort_values('timestamp'),
        quantum_df.sort_values('timestamp'),
        on='timestamp',
        tolerance=pd.Timedelta('1s'),
        direction='nearest'
    )
    
    return merged

# Example usage
hybrid_df = merge_quantum_classical_data(
    df_processed,
    'quantum_states.h5'
)</code></pre>
            </section>

            <section id="best-practices">
                <h2>Best Practices & Performance Optimization</h2>

                <h3>Memory Management</h3>
                <div class="subsection">
                    <ul>
                        <li>Use <code>dtype</code> parameter to specify appropriate data types (e.g., float32 vs float64)</li>
                        <li>Read data in chunks for large files: <code>pd.read_hdf(file, key, start=0, stop=1000)</code></li>
                        <li>Use categorical dtype for string columns to save memory</li>
                        <li>Delete unnecessary intermediate DataFrames: <code>del df_temp</code></li>
                    </ul>
                </div>

                <h3>I/O Performance</h3>
                <div class="subsection">
                    <ul>
                        <li>Use compression for storage efficiency: <code>complevel=9, complib='blosc'</code></li>
                        <li>Use chunked format for faster read/write: <code>format='table'</code></li>
                        <li>Index frequently accessed columns: <code>store.create_table_index(...)</code></li>
                        <li>Batch write operations instead of appending individual rows</li>
                    </ul>
                </div>

                <h3>Data Quality</h3>
                <div class="subsection">
                    <ul>
                        <li>Always validate data after loading with descriptive statistics</li>
                        <li>Check for missing values and handle appropriately</li>
                        <li>Verify metadata consistency before processing</li>
                        <li>Log all transformations for reproducibility</li>
                    </ul>
                </div>

                <h3>Error Handling</h3>
                <pre><code>import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    df = pd.read_hdf('data.h5', 'telemetry')
except FileNotFoundError:
    logger.error("HDF5 file not found")
    raise
except KeyError as e:
    logger.error(f"Dataset not found: {e}")
    raise
except Exception as e:
    logger.error(f"Unexpected error: {e}")
    raise</code></pre>
            </section>

            <section id="references">
                <h2>Resources & References</h2>

                <h3>Official Documentation</h3>
                <ul>
                    <li><a href="https://portal.hdfgroup.org/display/HDF5/HDF5" target="_blank">HDF5 Official Documentation</a></li>
                    <li><a href="https://hdfeos.org/" target="_blank">HDF-EOS Tools and Information Center</a></li>
                    <li><a href="https://pandas.pydata.org/" target="_blank">Pandas Documentation</a></li>
                    <li><a href="https://www.h5py.org/" target="_blank">h5py Documentation</a></li>
                </ul>

                <h3>NASA & Space Resources</h3>
                <ul>
                    <li><a href="https://www.earthdata.nasa.gov" target="_blank">NASA Earthdata</a></li>
                    <li><a href="https://data.nasa.gov/api/3" target="_blank">NASA Data API</a></li>
                    <li><a href="https://www.earthdata.nasa.gov/engage/open-data-services-software/earthdata-developer-portal" target="_blank">Earthdata Developer Portal</a></li>
                </ul>

                <h3>Related Technologies</h3>
                <ul>
                    <li><a href="https://www.elastic.co/integrations/data-integrations" target="_blank">Elastic Data Integrations</a></li>
                    <li><a href="https://cesium.com/platform/cesium-for-unreal/" target="_blank">Cesium for Unreal - 3D Visualization</a></li>
                    <li><a href="https://console.cloud.google.com/apis/library/earthengine.googleapis.com" target="_blank">Google Earth Engine</a></li>
                </ul>
            </section>


